const crypto = require('crypto');

const LogDAO = require('../dao/log');
const FileDAO = require('../dao/file');
const S3Service = require('../modules/s3');
const { mongo } = require('../config');

const s3Service = new S3Service();

module.exports.uploadFile = async (req, res) => {
  let key;
  let hash;

  try {
    if (!req.file) {
      return res.status(400).send('No file uploaded.');
    }

    hash = crypto.createHash('sha256').update(req.file.buffer).digest('hex');

    const existingHash = await FileDAO.findHashById(hash);

    if (existingHash) {
      await LogDAO.saveLog({
        fileHash: hash,
        message: 'File previously received. Event logged',
      });

      return res.status(409).send('File previously received. Event logged');
    }

    const uploadedFile = await s3Service.uploadFile({
      name: req.file.originalname,
      contentType: req.file.mimetype,
      buffer: req.file.buffer,
    });

    key = uploadedFile.Key;

    const fileData = {
      hash: hash,
      originalName: req.file.originalname,
      size: req.file.size,
      location: uploadedFile.Location,
    };

    await FileDAO.saveFileData(fileData);

    return res.status(201).send('File uploaded successfully');
  } catch (error) {
    console.error('Error uploading file:', error);

    /*
    If 2 or more requests come to the backend for a file that has not been uploaded before,
    the existingHash will not return true and the insert operation will try to perform 
    as many inserts as there are requests. But only 1 of them can be successful.
    The rest we need to delete from the bucket
    */
    if (error.code && error.code === mongo.duplicateErrorCode) {
      await LogDAO.saveLog({
        fileHash: hash,
        message: 'File already exists. Duplicate file not saved',
      });

      await s3Service.deleteFile(key);

      return res
        .status(409)
        .send('File already exists. Duplicate file not saved');
    }

    return res.status(500).send('Internal Server Error');
  }
};
